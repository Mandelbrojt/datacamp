---
title: "Introduction to Regression"
author: "Luis Moreno"
date: "11/2/2021"
output: html_document
---

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Import Required Packages}
library(fst)
library(ggplot2)
library(dplyr)
library(broom)
library(readr)
library(ggfortify)
library(yardstick)
```


```{r Import Required Datasets}
taiwan_real_estate <- read_fst("/Users/luismoreno/Documents/GitHub/datacamp/datasets/taiwan_real_estate.fst")
sp500_yearly_returns <- read_csv(file = "/Users/luismoreno/Documents/GitHub/datacamp/datasets/sp500_yearly_returns.csv")
ad_conversion <- read_csv(file = "/Users/luismoreno/Documents/GitHub/datacamp/datasets/ad_conversion.csv")
churn <- read_fst("/Users/luismoreno/Documents/GitHub/datacamp/datasets/churn.fst")
```

```{r Scatter Visualization}
ggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method="lm")
```

# Simple Linear Regression  
- Linear regression models always fit a straight line to the data. These are defined by two properties: their **intercept** and their **slope**.  

## Linear Regression Intercepts and Coefficients  
- The `lm()` function displays the two properties of a simple linear regression.  
- The syntax is as follows: `lm(formula = dependent_variable ~ independent_variable, data = dataframe)`.  

```{r Estimating a Linear Regression}
lin_reg <- lm(price_twd_msq ~ n_convenience, data = taiwan_real_estate)
lin_reg
```

## Categorical Explanatory Variables  
- If the **explanatory variable** is **categorical**, a histogram is a good option to visualize the data for each category.  
```{r Visualizing Categorical Data}
# Using taiwan_real_estate, plot price_twd_msq
ggplot(taiwan_real_estate, aes(price_twd_msq)) +
  # Make it a histogram with 10 bins
  geom_histogram(bins = 10) +
  # Facet the plot so each house age group gets its own panel
  facet_wrap(vars(house_age_years))
```

- A good way to explore categorical variables is to calculate summary statistics.  
```{r Summary Statistics by Category}
summary_stats <- taiwan_real_estate %>% 
  # Group by house age
  group_by(house_age_years) %>% 
  # Summarize to calculate the mean house price/area
  summarize(mean_by_group = mean(price_twd_msq))

# See the result
summary_stats
```

```{r Linear Regression with Categorical Variables}
# Run a linear regression of price_twd_msq vs. house_age_years
mdl_price_vs_age <- lm(price_twd_msq ~ house_age_years, data = taiwan_real_estate)

# See the result
mdl_price_vs_age

# Update the model formula to remove the intercept
mdl_price_vs_age_no_intercept <- lm(
  price_twd_msq ~ house_age_years + 0, 
  data = taiwan_real_estate
)

# See the result
mdl_price_vs_age_no_intercept
```

# Predictions and Model Objects  

## Making Predictions with Linear Regressions  
- To make a prediction with a linear regression you specify values for each of the explanatory variables, feed them to the model, and you get a prediction for the corresponding response variable.  

```{r Predict with Linear Regression}
# Create a tibble with n_convenience column from zero to ten
explanatory_data <- tibble(
  n_convenience = 0:10
)

# Run a linear regression of price_twd_msq vs. n_convenience
mdl_price_vs_conv <- lm(price_twd_msq ~ n_convenience,
                        data = taiwan_real_estate)

# Use mdl_price_vs_conv to predict with explanatory_data and create a new tibble containing both explanatory and prediction data
prediction_data <- explanatory_data %>% mutate(price_twd_msq = predict(mdl_price_vs_conv, explanatory_data))

# See the results
prediction_data

# Add the predicted data to the original scatter plot
ggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  # Add a point layer of prediction data, colored yellow
  geom_point(data = prediction_data, color = "red")
```

- You need to understand **what your data means** in order **to determine whether a prediction is nonsense or not**.  

## Extracting Linear Model Elements  
- The model coefficients, the fitted values, and the residuals are perhaps the most important bits of the linear regression model object.  

- `coefficients()`: it shows the code used to create the linear regression and the coefficients. It returns a named numeric vector of coefficients.  
- `fitted()`: is jargon for predictions on the original dataset used to create the model. It returns a numeric vector with one fit for each row of the dataset.  
- `residuals()`: is a measure of inaccuracy in the model fit. There is one residual for each row of the dataset used to create the linear regression.  
- `summary()`: shows a more extended printout of the details of the model.  
     - **call**: shows the code used to create the model.  
     - **residuals**: summary statistics of the residuals. The residuals should follow a normal distribution if the model is a good fit.  
     - **coefficients**: the first column are the values returned by the coefficients function. The last column are the p-values, which is the statistical significance.  
     - **model metrics**: shows metrics on the performance of the model.  

```{r Linear Model Elements}
# Get the model coefficients of mdl_price_vs_conv
coefficients(mdl_price_vs_conv)

# Get the first 6 fitted values of mdl_price_vs_conv
head(fitted(mdl_price_vs_conv))

# Get the first 6 residual values of mdl_price_vs_conv
head(residuals(mdl_price_vs_conv))

# Print a summary of mdl_price_vs_conv
summary(mdl_price_vs_conv)
```

### "Brooming" Model Elements  
The **`broom` package** contains functions that **decompose models into** three **data frames**:  
- **coefficient-level elements**: the coefficients themselves, as well as p-values for each coefficient;  
- **observation-level elements**: like fitted values and residuals;  
- **model-level elements**: mostly performance metrics.  

```{r Model Elements with Broom Package}
# Get the coefficient-level elements of the model
tidy(mdl_price_vs_conv)

# Get the first 6 observation-level elements of the model
head(augment(mdl_price_vs_conv))

glance(mdl_price_vs_conv)
```

## Regression to the Mean  
Roughly speaking, the regression to the mean concept means that **extreme cases do not persist over time**.  

```{r Plotting S&P 500 Returns}
# Using sp500_yearly_returns, plot return_2019 vs. return_2018
ggplot(sp500_yearly_returns, aes(return_2018, return_2019)) +
  # Make it a scatter plot
  geom_point() +
  # Add a line at y = x, colored green, size 1
  geom_abline(color = "green", size = 1) +
  # Add a linear regression trend line, no std. error ribbon
  geom_smooth(method = "lm", se = FALSE) +
  # Fix the coordinate ratio
  coord_fixed()

# Run a linear regression on return_2019 vs. return_2018
# using sp500_yearly_returns
mdl_returns <- lm(
  return_2019 ~ return_2018, 
  data = sp500_yearly_returns
)

# Create a data frame with return_2018 at -1, 0, and 1 
explanatory_data <- tibble(return_2018 = -1:1)

# Use mdl_returns to predict with explanatory_data
predict(mdl_returns, explanatory_data)
```

## Transforming Variables  
- If there is no straight line relationship between the response variable and the explanatory variable, it is sometimes possible to create one by transforming one or both of the variables.  

```{r Unstranformed Variable Plotting}
# Run the code to see the plot
# Edit so x-axis is square root of dist_to_mrt_m
ggplot(taiwan_real_estate, aes(dist_to_mrt_m, price_twd_msq)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

```{r Transforming Explanatory Variable}
# Run the code to see the plot
# Edit so x-axis is square root of dist_to_mrt_m
ggplot(taiwan_real_estate, aes(sqrt(dist_to_mrt_m), price_twd_msq)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

# Run a linear regression of price_twd_msq vs. 
# square root of dist_to_mrt_m using taiwan_real_estate
mdl_price_vs_dist <- lm( price_twd_msq ~ sqrt(dist_to_mrt_m), data = taiwan_real_estate)

# See the result
mdl_price_vs_dist

# Use this explanatory data
explanatory_data <- tibble(
  dist_to_mrt_m = seq(0, 80, 10) ^ 2
)

# Use mdl_price_vs_dist to predict explanatory_data
prediction_data <- explanatory_data %>% mutate(price_twd_msq = predict(mdl_price_vs_dist, explanatory_data))

# See the result
prediction_data

# Compare with the final plot
ggplot(taiwan_real_estate, aes(sqrt(dist_to_mrt_m), price_twd_msq)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  # Add points from prediction_data, colored green, size 5
  geom_point(data = prediction_data, color ="green", size=5)
```

- By transforming the explanatory variable, the relationship with the response variable became linear, and so a linear regression became an appropriate model.  

- The **response variable can also be transformed**, but this means you need an extra step at the end to undo that transformation, that is, you **"back transform" the predictions**.  

```{r Transforming Response Variable}
# Edit the plot to raise x, y aesthetics to power 0.25
ggplot(ad_conversion, aes(n_impressions ^ 0.25, n_clicks ^0.25)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

# Run a linear regression of n_clicks to the power 0.25 versus n_impressions to the power 0.25 using ad_conversion. 
# Each variable in the formula needs to be specified "as is", using I().
mdl_click_vs_impression <- lm(I(n_clicks ^ 0.25) ~ I(n_impressions ^ 0.25), data = ad_conversion)

# Use this explanatory data
explanatory_data <- tibble(
  n_impressions = seq(0, 3e6, 5e5)
)

# Use mdl_click_vs_impression to predict n_clicks to the power 0.25 from explanatory_data.
prediction_data <- explanatory_data %>% 
  mutate(
    # Use mdl_click_vs_impression to predict n_clicks ^ 0.25
    n_clicks_025 = predict(mdl_click_vs_impression, explanatory_data),
    # Back transform to get n_clicks
    n_clicks = n_clicks_025 ^ 4
  )

# Add a layer of points from prediction_data colored in green
ggplot(ad_conversion, aes(n_impressions ^ 0.25, n_clicks ^ 0.25)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  # Add points from prediction_data, colored green
  geom_point(data = prediction_data, color = "green")
```

# Assesing Model Fit  

## Coefficient of Determination  

- The coefficient of determination is a measure of how well the linear regression line fits the observed values.  

- It is commonly known as `R-squared`, and is sometimes referred to as "goodness of fit".  

- This measure is represented as a value **between 0 and 1**.  

- For a simple linear regression, it is equal to the **square of the correlation between the explanatory and response variables**.  

```{r Creating Simple Linear Models}
# Create two linear regression models to compare its coefficients
mdl_click_vs_impression_orig <- lm(n_clicks ~ n_impressions, data = ad_conversion)
mdl_click_vs_impression_exp <- lm(I(n_clicks ^ 0.25) ~ I(n_impressions ^ 0.25), data = ad_conversion)

# Print the summary of each linear model
summary(mdl_click_vs_impression_orig)
summary(mdl_click_vs_impression_exp)
```

```{r Comparing Coefficients of Determination}
# Get coeff of determination for mdl_click_vs_impression_orig
mdl_click_vs_impression_orig %>% 
  # Get the model-level details
  glance() %>% 
  # Pull out r.squared
  pull(r.squared)

# Get coeff of determination for mdl_click_vs_impression_exp
mdl_click_vs_impression_exp %>% 
  # Get the model-level details
  glance() %>% 
  # Pull out r.squared
  pull(r.squared)
```

- The number of impressions (independent variable) explains 89% of the variability in the number of clicks (dependent variable) for the first linear model.  

- **Conclusion**: The independent variable of the transformed linear model explains better the variability of the dependent variable.

## Residual Standard Error  

- The residual standard error (RSE), is a measure of the typical size of the residuals. It also measures how badly wrong you can expect predictions to be.  

- The closer the value of the RSE to 0, the better the model fits to the data.  

- In a nutshell, **RSE is a measure of accuracy for regression models and regression trees**.  

```{r Comparing Residual Standard Errors}
# Get RSE for mdl_click_vs_impression_orig
mdl_click_vs_impression_orig %>% 
  # Get the model-level details
  glance() %>% 
  # Pull out sigma
  pull(sigma)

# Get RSE for mdl_click_vs_impression_exp
mdl_click_vs_impression_exp %>% 
  # Get the model-level details
  glance() %>% 
  # Pull out sigma
  pull(sigma)
```

- The typical difference between observed number of clicks (dependent variable) and predicted number of clicks is 20 for the first linear model.  

- **Conclusion**: RSE suggest that the transformed model gives more accurate predictions.  

## Visualizing Model Fit  

### Residuals vs. Fitted Values  

```{r Residual vs Fitted Plot}
autoplot(mdl_price_vs_conv, which = 1, nrow = 1, ncol = 1)
```

- It shows a LOESS trend line following the data, which is used for visualizing the trend but not for making predictions.  
- **Interpretation**: if residuals are normally distributed with mean zero, then the trend line should closely follow the `y = 0` line on the plot. A good linear model should have a trend line close to zero.  

### Normal Q-Q  

```{r Q-Q Plot}
autoplot(mdl_price_vs_conv, which = 2, nrow = 1, ncol = 1)
```

- It shows whether or not the residuals follow a normal distribution.  
- `x-axis` = quantiles from the normal distribution.  
- `y-axis` = standardized residuals, which are the residuals divided by their standard deviation.  
- **Interpretation**: if the points of the plot track along the straight line, they are normally distributed, if not, they aren't.  

### Scale Location  
```{r Scale Location Plot}
autoplot(mdl_price_vs_conv, which = 3, nrow = 1, ncol = 1)
```

- It shows the square root of the standardized residuals versus the fitted values.  
- It shows whether the size of the residuals gets bigger or smaller as the fitted values change.  
- **Interpretation**: in a good linear model, the size of the residuals shouldn't change much as the fitted values change.  

## Outliers, Leverage, and Influence  

### Leverage  

- Leverage measures **how unusual or extreme the explanatory variables are** for each observation.  
- A **high leverage** means that the **explanatory variable has values that are different to other points in the dataset**.  
- A **high leverage** in a simple linear regression model, means **values with a very high or very low explanatory value**.  
- Observations with a large distance to the explanatory variable have the highest leverage, because most of the observations are near, so long distances are more extreme.

### Influence  

- Influence measures **how much a model would change if each observation was left out of the model calculations, one at a time**.  
- The **standard metric for influence is Cook's distance**, which calculates influence based on the size of the residual and the leverage of the point.  
- Observations with predictions far away from the trend line have high influence, because they have large residuals and are far away from other observations.  

```{r Influence and Leverage}
mdl_price_vs_dist %>% 
  # Augment the model
  augment() %>% 
  # Arrange rows by descending leverage
  arrange(desc(.hat)) %>% 
  # Get the head of the dataset
  head()

mdl_price_vs_dist %>% 
  # Augment the model
  augment() %>% 
  # Arrange rows by descending Cook's distance
  arrange(desc(.cooksd)) %>% 
  # Get the head of the dataset
  head()

# Plot the three outlier diagnostics for mdl_price_vs_conv
autoplot(
    mdl_price_vs_dist,
    which = 4:6,
    nrow = 3,
    ncol = 1)
```

# Simple Logistic Regression  
- Logistic regressions are used when the response variable is logical.  
- The responses follow logistic curve (S-shape).  

```{r Visualizing Binomial Variables}
# Using churn, plot time_since_last_purchase
ggplot(churn, aes(time_since_last_purchase)) +
  # as a histogram with binwidth 0.25
  geom_histogram(binwidth = 0.25) +
  # faceted in a grid with has_churned on each row
  facet_grid(rows = vars(has_churned))

# Redraw the plot with time_since_first_purchase
ggplot(churn, aes(time_since_first_purchase)) + geom_histogram(binwidth = 0.25) + facet_grid(rows = vars(has_churned))
```

```{r Visualizing Linear and Logistic Models}
# Using churn plot has_churned vs. time_since_first_purchase
ggplot(churn, aes(time_since_first_purchase, has_churned)) +
  # Make it a scatter plot
  geom_point() +
  # Add an lm trend line, no std error ribbon, colored red
  geom_smooth(method="lm",se=FALSE,color="red") +
  # Add a glm trend line, no std error ribbon, binomial family
  geom_smooth(method="glm",se = FALSE,method.args=list(family = binomial))
  
```

- Linear regression and logistic regression are **special cases** of a broader type of models called **generalized linear models** ("GLMs").  
- A **linear regression** makes the assumption that the **residuals** follow a **Gaussian distribution**.  
- A **logistic regression** assumes that **residuals** follow a **binomial distribution**.  

```{r Creating a Logistic Regression}
# Fit a logistic regression of churn vs. 
# length of relationship using the churn dataset
mdl_churn_vs_relationship <- glm(has_churned ~ time_since_first_purchase, data = churn, family = binomial)

# See the result
mdl_churn_vs_relationship
```

## Expressing Predictions    

### Probabilities  
- Since the response variable is either "yes" or "no", you can make a prediction of the probability of a "yes".  

```{r Predicting with Probabilities}

plt_churn_vs_relationship <- 
  # Using churn plot has_churned vs. time_since_first_purchase
ggplot(churn, aes(time_since_first_purchase, has_churned)) +
  # Make it a scatter plot
  geom_point() +
  # Add a glm trend line, no std error ribbon, binomial family
  geom_smooth(method="glm",se = FALSE,method.args=list(family = binomial))

# Create the explanatory data
explanatory_data <- tibble(time_since_first_purchase = seq(-1, 6, 0.25))

# Make a data frame of predicted probabilities
prediction_data <- explanatory_data %>% 
  mutate(
    has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = "response")
  )

# Update the plot
plt_churn_vs_relationship +
  # Add points from prediction_data, colored yellow, size 2
  geom_point(data = prediction_data, color = "yellow", size = 2)
```

### Most Likely Outcome  
- When explaining your results to a non-technical audience simply explain the most likely outcome. The trade-off here is easier interpretation at the cost of nuance.  

```{r Predicting with Most Likely Outcome}
# Update the data frame
prediction_data <- explanatory_data %>% 
  mutate(   
    has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = "response"),
    # Add the most likely churn outcome
    most_likely_outcome = round(has_churned)
  )

# Update the plot
plt_churn_vs_relationship +
  # Add most likely outcome points from prediction_data, 
  # colored yellow, size 2
  geom_point(aes(y = most_likely_outcome), data = prediction_data, color = "yellow", size = 2)
```

### Odds Ratio  
- Odds ratios compare the probability of something happening with the probability of it not happening. This is sometimes easier to reason about than probabilities, particularly when you want to make decisions about choices.  

```{r Predicting with Odds Ratio}
# Update the data frame
prediction_data <- explanatory_data %>% 
  mutate(   
    has_churned = predict(
      mdl_churn_vs_relationship, explanatory_data, 
      type = "response"
    ),
    # Add the odds ratio
    odds_ratio = has_churned / (1 - has_churned)
  )

# Using prediction_data, plot odds_ratio vs. time_since_first_purchase
ggplot(prediction_data, aes(time_since_first_purchase, odds_ratio)) +
  # Make it a line plot
  geom_line() +
  # Add a dotted horizontal line at y = 1
  geom_hline(yintercept = 1, linetype = "dotted")
```

### Log Odds Ratio  
- One **downside** to **probabilities and odds ratios for logistic regression** predictions is that the prediction **lines** for each **are curved**. This makes it **harder to reason about what happens to the prediction when you make a change to the explanatory variable**.  
- The logarithm of the odds ratio (the "log odds ratio") does have a linear relationship between predicted response and explanatory variable.  
- It's usually better to plot the odds ratio and apply a log transformation to the y-axis scale.  

```{r Predicting with Log Odds Ratio}
# Update the data frame
prediction_data <- explanatory_data %>% 
  mutate(   
    has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = "response"),
    odds_ratio = has_churned / (1 - has_churned),
    # Add the log odds ratio from odds_ratio
    log_odds_ratio = log(odds_ratio),
    # Add the log odds ratio using predict()
    log_odds_ratio2 = predict(mdl_churn_vs_relationship, explanatory_data)
  )

# Update the plot
ggplot(prediction_data, aes(time_since_first_purchase, odds_ratio)) +
  geom_line() +
  geom_hline(yintercept = 1, linetype = "dotted") +
  # Use a logarithmic y-scale
  scale_y_log10()
```

## Quantifying Logistic Regression Fit  

## Confusion Matrix  
- It is the basis of all performance metrics for **models with a categorical response**.
- It contains the **counts of each actual response-predicted response pair**. 
- There are 4 values in the confusion matrix:
  - 
```{r Creating a Confusion Matrix}
# Get the actual responses from the dataset
actual_response <- churn$has_churned

# Get the "most likely" responses from the model
predicted_response <- round(fitted(mdl_churn_vs_relationship))

# Create a table of counts
outcomes <- table(predicted_response, actual_response)

# See the result
outcomes
```

### Measuring Performance Metrics  
- By converting this to a `yardstick` **confusion matrix object**, you get methods for plotting and extracting performance metrics.  
- For **logistic regression**, three `yardstick` objects are important: **accuracy, sensitivity, and specificity**.  

```{r Visualizing Performance Metrics}
# Convert outcomes to a yardstick confusion matrix
confusion <-  conf_mat(outcomes)

# Plot the confusion matrix
autoplot(confusion)

# Get performance metrics for the confusion matrix
summary(confusion, event_level = "second")
```

