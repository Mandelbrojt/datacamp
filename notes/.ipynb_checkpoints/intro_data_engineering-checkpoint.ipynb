{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb206b1-4296-4852-b516-54af02c015c2",
   "metadata": {},
   "source": [
    "# Data Engineering Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b2720-a0b4-4dee-a830-d546848ee103",
   "metadata": {},
   "source": [
    "## Databases\n",
    "\n",
    "**Definition**: A database is a computer system that holds large amounts of data.\n",
    "\n",
    "- The data engineer's task begins and ends at databases or other raw data formats.\n",
    "    - **MySQL**\n",
    "    - **PostgreSQL**\n",
    "    \n",
    "### Data Structures\n",
    "\n",
    "- Structured data\n",
    "    - Relational database\n",
    "- Semi-structured data\n",
    "    - JSON\n",
    "- Unstructured data\n",
    "    - Photographs or videos\n",
    "\n",
    "### Types of Databases\n",
    "\n",
    "- SQL: relational database.\n",
    "    - MySQL\n",
    "    - PostgreSQL\n",
    "- NoSQL: no relational database.\n",
    "    - redis\n",
    "    - mongoDB\n",
    "\n",
    "### Database Schema\n",
    "\n",
    "- Describes the structure and relations of a database.\n",
    "- A foreign key connects tables.\n",
    "- The star schema consists of one or more fact tables referencing any number of dimension tables.\n",
    "    - Fact tables: contain records that represent things that happened in the world.\n",
    "    - Dimension tables: hold information on the world itself.\n",
    "    \n",
    "#### Queriying a database with `pandas` with `read_sql()` function\n",
    "```python\n",
    "# Store data in a pandas dataframe\n",
    "data = pd.read_sql(\"\"\"\n",
    "SELECT first_name, last_name FROM \"Customer\"\n",
    "ORDER BY last_name, first_name\n",
    "\"\"\", db_engine)\n",
    "\n",
    "# Show the first 3 rows of the DataFrame\n",
    "print(data.head(3))\n",
    "\n",
    "# Show the info of the DataFrame\n",
    "print(data.info())\n",
    "```\n",
    "\n",
    "#### Joining Tables in SQL with Python\n",
    "```python\n",
    "# Join two tables with JOIN statement\n",
    "data = pd.read_sql(\"\"\"\n",
    "SELECT * FROM \"Customer\"\n",
    "INNER JOIN \"Order\"\n",
    "ON \"Order\".\"customer_id\"=\"Customer\".\"id\"\n",
    "\"\"\", db_engine)\n",
    "\n",
    "# Show the id column of data\n",
    "print(data.id)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedbdab3-f63d-47c7-be8b-bf3e4e826b8d",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "- Data is joined, cleaned, or organized in parallel by using clusters of machines before loading it into a destination analytics database.\n",
    "    - **Spark**\n",
    "    - **Hive**\n",
    "\n",
    "### Frameworks\n",
    "\n",
    "- [Apache Hadoop](https://hadoop.apache.org/): is a **framework** that allows for the distributed **processing** of **large** **data** **sets** across **clusters** of **computers** using simple programming models\n",
    "    - [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html#Introduction): is a distributed file system designed to run on commodity hardware.\n",
    "    - **MapReduce**: software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware.\n",
    "    - [Hive](https://hive.apache.org/): facilitates reading, writing, and managing large datasets residing in distributed storage using SQL.\n",
    "\n",
    "\n",
    "- [Apache Spark](https://spark.apache.org/): is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.\n",
    "    - Relies on **Resilient Distributed Datasets**. These are similar as list of tuples.\n",
    "    - Transformations are done with `.map()` or `.filter()`.\n",
    "    - Actions are done with `.count()` or `.first().`\n",
    "\n",
    "\n",
    "- [PySpark](https://spark.apache.org/docs/latest/api/python/index.html): write Spark applications using Python APIs, or the PySpark shell for interactively analyzing your data in a distributed environment.\n",
    "\n",
    "### Parallel Computing\n",
    "\n",
    "Main uses:\n",
    "- Split a task into sub-tasks\n",
    "- Distribute sub-tasks over several computers\n",
    "- You can't split every task successfully into subtasks\n",
    "- Some tasks might be too small to benefit from parallel computing due to the communication overhead\n",
    "\n",
    "```python\n",
    "# Time each operation\n",
    "@print_timing\n",
    "\n",
    "# Function to apply a function over multiple cores\n",
    "def parallel_apply(apply_func, groups, nb_cores):\n",
    "    with Pool(nb_cores) as p:\n",
    "        results = p.map(apply_func, groups)\n",
    "    return pd.concat(results)\n",
    "\n",
    "# Parallel apply using 1 core\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 1)\n",
    "\n",
    "# Parallel apply using 2 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 2)\n",
    "\n",
    "# Parallel apply using 4 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 4)\n",
    "\n",
    "\"\"\"\n",
    "Processing time: 931.4761161804199\n",
    "Processing time: 619.7404861450195\n",
    "Processing time: 349.50923919677734\n",
    "\"\"\"\n",
    "````\n",
    "\n",
    "- the `multiprocessor.Pool` API allows you to distribute your workload over several processes.\n",
    "- `@print_timing` decorator is used to time each operation.\n",
    "\n",
    "- A more convenient way to parallelize an apply over several groups is using the `dask` framework and its abstraction of the `pandas` DataFrame\n",
    "\n",
    "#### Using DataFrame abstraction for parallel computing\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Set the number of partitions\n",
    "athlete_events_dask = dd.from_pandas(athlete_events, npartitions=4)\n",
    "\n",
    "# Calculate the mean Age per Year\n",
    "print(athlete_events_dask.groupby('Year').Age.mean().compute())\n",
    "\n",
    "\"\"\"\n",
    "...\n",
    "2012    25.961378\n",
    "2014    25.987324\n",
    "2016    26.207919\n",
    "Name: Age, dtype: float64\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab02e1e-072b-47d3-b9f3-06b0257101cd",
   "metadata": {},
   "source": [
    "## Scheduling\n",
    "\n",
    "- Scheduling tools moves data from one place to another at the correct time, with a specific interval.\n",
    "    - **Apache Airflow**\n",
    "    - **Oozie**\n",
    "    - **bash tool: cron**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476af3c-dfbd-4d3e-8fce-d1ebe4724218",
   "metadata": {},
   "source": [
    "## Cloud Computing\n",
    "\n",
    "- In the cloud, you use the resources you need, at the time you need them. It is a way of cost optimization that gives database reliability by also solving logistical problems.\n",
    "\n",
    "### Cloud Providers\n",
    "\n",
    "- [Amazon Web Services](https://aws.amazon.com/)\n",
    "    - S3\n",
    "    - EC2\n",
    "    - RDS\n",
    "- [Microsoft Azure](https://azure.microsoft.com/)\n",
    "    - Blob Storage\n",
    "    - Virtual Machines\n",
    "    - SQL Database\n",
    "- [Google Cloud](https://cloud.google.com/)\n",
    "    - Cloud Storage\n",
    "    - Compute Engine\n",
    "    - Cloud SQL\n",
    "\n",
    "### Cloud Services\n",
    "\n",
    "- **Storage**: store reliably all types of files to the cloud.\n",
    "- **Computation**: perform computations on the cloud via virtual machines.\n",
    "- **Databases**: store large amounts of data organized for rapid search and retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
