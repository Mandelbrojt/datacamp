---
title: "Foundations of Probability in R"
author: "Luis M."
date: "23/07/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r packages}
library(tidyverse)
```

# Binomial Distribution

## Simulations
```{r Binomial Simulations}
# Generate 10 separate random flips with probability .3
rbinom(10, 1, 0.3)

# Generate 5 occurrences of flipping 10 coins, each with 30% probability
rbinom(5, 10, 0.3)
```

## Density
Question: If you flip 10 coins each with a 30% probability of coming up heads, what is the probability exactly 2 of them are heads?
```{r Binomial Density}
# Calculate the probability that 2 coins are heads
dbinom(2, 10, .3)

# Confirm the probability with the average
mean(rbinom(10000, 10, .3) == 2)
```

## Cumulative Density
**Question**: If you flip 10 coins that each have a 30% probability of heads, what is the probability at least 5 are heads?
```{r Binomial Cumulative Density}
# Calculate the probability that at least five coins are heads
1 - pbinom(4,10, .3)

# Confirm your answer with a simulation of 10,000 trials
mean(rbinom(10000, 10, .3) >= 5)

# Try now with 100, 1000, 10,000, and 100,000 trials
mean(rbinom(100, 10, .3) >= 5)
mean(rbinom(1000, 10, .3) >= 5)
mean(rbinom(10000, 10, .3) >= 5)
mean(rbinom(100000, 10, .3) >= 5)
```

## Expected Value and Variance  
* The expected value shows **where the distribution is centered**.  

* The variance shows **how the distribution is spread**. 

**Question**: What is the **expected value** of a binomial distribution where 25 coins are flipped, each having a 30% chance of heads?

```{r Calculating Expected Values}
# Calculate the expected value using the exact formula
25 * .3

# Confirm with a simulation using rbinom
mean(rbinom(10000, 25, .3))
```

```{r Calculating Variance}
# Calculate the variance using the exact formula
25 * .3 * (1 - .3)

# Confirm with a simulation using rbinom
var(rbinom(10000, 25, .3))
```

# Laws of Probability  

## Probability of Independent Events  

### Conjunction  

**Question**: If events A and B are independent, and A has a 40% chance of happening, and event B has a 20% chance of happening, what is the probability they will both happen?

```{r Independent Probabilities by Conjunction}
# Simulate 100,000 flips of a coin with a 40% chance of heads
A <- rbinom(100000, 1, .4)

# Simulate 100,000 flips of a coin with a 20% chance of heads
B <- rbinom(100000, 1, .2)

# Estimate the probability both A and B are heads
mean(A & B)

# Simulate 100,000 flips of coin C (70% chance of heads)
C <- rbinom(100000, 1, .7)

# Estimate the probability A, B, and C are all heads
mean(A & B & C)
```

### Disjunction  

**Question**: If coins A and B are independent, and A has a 60% chance of coming up heads, and event B has a 10% chance of coming up heads, what is the probability either A or B will come up heads?  

```{r Independent Probabilities by Disjunction}
# Simulate 100,000 flips of a coin with a 60% chance of heads
A <- rbinom(100000, 1, .6)

# Simulate 100,000 flips of a coin with a 10% chance of heads
B <- rbinom(100000, 1, .10)

# Estimate the probability either A or B is heads
mean(A | B)
```

Suppose X is a random variable (10 flips of a coin with 60% chance of heads) and Y is a random variable (10 flips of a coin with a 70% chance of heads), and they are **independent**.  

**Question**: What is the probability that either of the variables is **less than or equal** to 4?

```{r Independent Probabilities by Disjunction II}
# Use rbinom to simulate 100,000 draws from each of X and Y
X <- rbinom(100000, 10, .6)
Y <- rbinom(100000, 10, .7)

# Estimate the probability either X or Y is <= to 4
mean(X <= 4 | Y <= 4)

# Use pbinom to calculate the probabilities separately
prob_X_less <- pbinom(4, 10, .6)
prob_Y_less <- pbinom(4, 10, .7)

# Combine these to calculate the exact probability either <= 4
prob_X_less + prob_Y_less - (prob_X_less * prob_Y_less)
```

### Multiplying Independent Random Variables  
```{r Random Variable Multiplication - Mean}
# Simulate 100,000 draws of a binomial with size 20 and p = .1
X <- rbinom(100000, 20, .1)

# Estimate the expected value of X
mean(X)

# Estimate the expected value of 5 * X
mean(5 * X)
```

```{r Random Variable Multiplication - Variance}
# X is simulated from 100,000 draws of a binomial with size 20 and p = .1
X <- rbinom(100000, 20, .1)

# Estimate the variance of X
var(X)

# Estimate the variance of 5 * X
var(5 * X)
```

### Adding Independent Random Variables  
**Question**: If X is drawn from a binomial with size 20 and p = .3, and Y from size 40 and p = .1, what is the expected value (mean) of X + Y?  

```{r Sum of Binomial Variables}
# Simulate 100,000 draws of X (size 20, p = .3) and Y (size 40, p = .1)
X <- rbinom(100000, 20, .3)
Y <- rbinom(100000, 40, .1)

# Estimate the expected value of X + Y
mean(X + Y)

# Find the variance of X + Y
var(X + Y)

# Find the variance of 3 * X + Y
var(3 * X + Y)
```

# Bayesian Statistsics  

## Updating Probabilities with Evidence  

We see **11 out of 20 flips** from a coin that is either fair (50% chance of heads) or biased (75% chance of heads). How likely is it that the coin is fair?

```{r Updating Probabilities with Simulations}
# Simulate 50000 cases of flipping 20 coins from fair and from biased
fair <- rbinom(50000, 20, .5)
biased <- rbinom(50000, 20, .75)

# How many fair cases, and how many biased, led to exactly 11 heads?
fair_11 <- sum(fair == 11)
biased_11 <- sum(biased == 11)

# Find the fraction of fair coins that are 11 out of all coins that were 11
fair_11 / (fair_11 + biased_11)
```

```{r Updating Probabilities with Simulations II}
# Simulate 50000 cases of flipping 20 coins from fair and from biased
fair <- rbinom(50000, 20, .5)
biased <- rbinom(50000, 20, .75)

# How many fair cases, and how many biased, led to exactly 16 heads?
fair_16 <- sum(fair == 16)
biased_16 <- sum(biased == 16)

# Find the fraction of fair coins that are 16 out of all coins that were 16
fair_16 / (fair_16 + biased_16)
```

## Prior Probabilities  

```{r Updating Prior Probabilities I}
# Simulate 8000 cases of flipping a fair coin, and 2000 of a biased coin
fair_flips <- rbinom(8000, 20, .5)
biased_flips <-rbinom(2000, 20, .75)

# Find the number of cases from each coin that resulted in 14/20
fair_14 <- sum(fair_flips == 14)
biased_14 <- sum(biased_flips == 14)

# Use these to estimate the posterior probability
fair_14 / (biased_14 + fair_14)
```

**Question**: Suppose instead of a coin being either fair or biased, there are three possibilities: that the coin is fair (50% heads), low (25% heads), and high (75% heads). There is a 80% chance it is fair, a 10% chance it is biased low, and a 10% chance it is biased high.

You see 14/20 flips are heads. What is the probability that the coin is fair?

```{r Updating Prior Probabilities II}
# Simulate 80,000 draws from fair coin, 10,000 from each of high and low coins
flips_fair <- rbinom(80000, 20, .5)
flips_high <- rbinom(10000, 20, .75)
flips_low <- rbinom(10000, 20, .25)

# Compute the number of coins that resulted in 14 heads from each of these piles
fair_14 <- sum(flips_fair == 14)
high_14 <- sum(flips_high == 14)
low_14 <- sum(flips_low == 14)

# Compute the posterior probability that the coin was fair
fair_14 / (fair_14 + high_14 + low_14)
```

## Bayes Theorem  

The **Bayes Theorem** describes the probability of an event, based on prior knowledge of **conditions** that might be related to the event.  

The Bayesian probability interpretation expresses **how a degree of belief**, expressed as a probability, **should rationally change** to account for the availability of related **evidence**.  

$\frac{P(B \mid A) Pr(A)}{Pr(B \mid A) Pr(A) + Pr(B \mid ¬ A)Pr(¬A)}$  

**Conditional probability**: is informally defined as the probability of one event given the occurrence of another event.  

**Question**: calculate the exact probability of getting 11 heads out of 20 flips with a fair coin (50% chance of heads) and with a biased coin (75% chance of heads).  

```{r Updating Probabilities with Prior Probabilities I}
# Use dbinom to calculate the probability of 11/20 heads with fair or biased coin
probability_fair <- dbinom(11, 20, .5)
probability_biased <- dbinom(11, 20, .75)

# Calculate the posterior probability that the coin is fair
probability_fair / (probability_fair + probability_biased)
```

* Assuming that beforehand there was an equal chance of it being a fair coin or a biased coin. Find the posterior probability if there were two other outcomes.  

```{r Updating Probabilities with Prior Probabilities II}
# Find the probability that a coin resulting in 14/20 is fair
dbinom(14, 20, .5) / (dbinom(14, 20, .5) + dbinom(14, 20, .75))

# Find the probability that a coin resulting in 18/20 is fair
dbinom(18, 20, .5) / (dbinom(18, 20, .5) + dbinom(18, 20, .75))
```

* Suppose we had set a prior probability of a 99% chance that the coin is fair (50% chance of heads), and only a 1% chance that the coin is biased (75% chance of heads).

$Pr(\mbox{fair}|A)=\frac{\Pr(A|\mbox{fair})\Pr(\mbox{fair})}{\Pr(A|\mbox{fair})\Pr(\mbox{fair})+\Pr(A|\mbox{biased})\Pr(\mbox{biased})}$

```{r Updating Probabilities with Bayes Theorem}
# Use dbinom to find the probability of 16/20 from a fair or biased coin
probability_16_fair <- dbinom(16, 20, .5)
probability_16_biased <- dbinom(16, 20, .75)

# Use Bayes' theorem to find the posterior probability that the coin is fair
probability_16_fair * .99 / ((probability_16_fair * .99) + (probability_16_biased * .01))
```

## Related Distributions  

### Normal Distribution  

Normal distributions have the following properties:  

* symmetric bell shape,  
* **mean** and **median** are equal; both located at the center of the distribution,  
* Approximately 68% of the data falls within 1 standard deviation of the mean,  
* Approximately 95% of the data falls within 2 standard deviations of the mean,  
* Approximately 99.7% of the data falls within 3 standard deviations of the mean.  


Suppose you flipped 1000 coins, each with a 20% chance of being heads. What would be the mean and variance of the binomial distribution?

```{r Approximating a Binomial to the Normal}
mean <- 1000 * 0.2
variance <- 1000 * 0.2 * (1 - 0.2)
paste0("The mean is: ",as.character(mean))
paste0("The variance is: ",as.character(variance))
```

```{r Simulating from the Binomial and the Normal}
# Set the number of observations
n_obs <- 100000

# Set the number of trials
n_trials <- 1000

# Set the probability of success on each trial
pr_success <- .2

# Draw a random sample of 100,000 from the Binomial(1000, .2) distribution
binom_sample <- rbinom(n_obs, n_trials, pr_success)

# Draw a random sample of 100,000 from the normal approximation
normal_sample <- rnorm(n_obs, (n_trials*pr_success), sqrt((n_trials*pr_success)*(1-pr_success)))

# Create a data frame containing the type of distribution and the value of each type of distribution
df <- data.frame(distribution_type = c(rep("binom_sample",length(binom_sample)), rep("normal_sample",length(normal_sample))), samples = c(binom_sample,normal_sample))

library(ggplot2)

# Set the bin-width based on Freedman-Diaconis rule
num_bins <- 2 * IQR(normal_sample) / length(normal_sample)^(1/3)

# Plot two histograms for comparing each distribution type
ggplot(data = df, aes(x=samples)) +
     geom_histogram(binwidth = num_bins) +
     facet_wrap(~distribution_type)
```

* **Question**: If you flip 1000 coins that each have a 20% chance of being heads, what is the probability you would get 190 heads or fewer?

```{r Comparing Cumulative Density of the Binomial}
# Simulations from the normal and binomial distributions
binom_sample <- rbinom(100000, 1000, .2)
normal_sample <- rnorm(100000, 200, sqrt(160))

# Use binom_sample to estimate the probability of <= 190 heads
mean(binom_sample <= 190)

# Use normal_sample to estimate the probability of <= 190 heads
mean(normal_sample <= 190)

# Calculate the probability of <= 190 heads with pbinom
pbinom(190, 1000, .2)

# Calculate the probability of <= 190 heads with pnorm
pnorm(190, 200, sqrt(160))
```

### Poisson Distribution  
```{r Simulating from a Poisson and Binomial}
n_obs <- 100000
n_trials <- 1000
probability <- .002

# Draw a random sample of 100,000 from the Binomial(1000, .002) distribution
binom_sample <- rbinom(n_obs, n_trials, probability)

# Draw a random sample of 100,000 from the Poisson approximation
poisson_sample <- rpois(n_obs, n_trials * probability)

# Create a data frame containing the type of distribution and the value of each type of distribution
df <- data.frame(distribution_type = c(rep("binom_sample",length(binom_sample)), rep("poisson_sample",length(poisson_sample))), samples = c(binom_sample,poisson_sample))

library(ggplot2)

# Set the bin-width based on Freedman-Diaconis rule
num_bins <- 2 * IQR(poisson_sample) / length(poisson_sample)^(1/3)

# Plot two histograms for comparing each distribution type
ggplot(data = df, aes(x=samples)) +
     geom_histogram(binwidth = num_bins) +
     facet_wrap(~distribution_type)
```

```{r Density of the Poisson Distribution}
# Simulate 100,000 draws from Poisson(2)
poisson_sample <- rpois(100000, 2)

# Find the percentage of simulated values that are 0
sum(poisson_sample == 0) / length(poisson_sample)

# Use dpois to find the exact probability that a draw is 0
dpois(0, 2)
```

* When you **add multiple Poisson distributions** together, the **result is also a Poisson distribution**.  

```{r Sum of Two Poisson Variables}
# Simulate 100,000 draws from Poisson(1)
W <- rpois(100000, 1)

# Simulate 100,000 draws from Poisson(2)
X <- rpois(100000, 2)

# Simulate 100,000 draws from Poisson(3)
Y <- rpois(100000, 3)

# Add X and Y together to create Z
Z <- W + X

# Create a data frame containing the type of distribution and the value of each type of distribution
df <- data.frame(distribution_type = c(rep("Y",length(Y)),
                                       rep("Z",length(Z))),
                 samples = c(Y, Z))

library(ggplot2)

# Set the bin-width based on Freedman-Diaconis rule
num_bins <- 2 * IQR(Z) / length(Z)^(1/3)

# Plot two histograms for comparing each distribution type
ggplot(data = df, aes(x=samples)) +
     geom_histogram(binwidth = num_bins) +
     facet_wrap(~distribution_type)
```

### Geometric Distribution  
```{r Waiting for Events}
# Simulate 100 instances of flipping a 20% coin
flips <- rbinom(100, 1, .2)

# Use which to find the first case of 1 ("heads")
which(flips == 1)[1]
```

```{r Using replicate() for Simulations}
# Replicate this 100,000 times using replicate()
replications <- replicate(100000, 
                          which(rbinom(100, 1, 0.2) == 1)[1])

# Histogram the replications with qplot
qplot(replications)
```

```{r Simulating from the Geometric Distribution}
# Use replication to generate the geometric series
replications <- replicate(100000, which(rbinom(100, 1, .2) == 1)[1])

# Generate 100,000 draws from the corresponding geometric distribution
geom_sample <- rgeom(100000, .2)

# Create a data frame containing the type of distribution and the value of each type of distribution
df <- data.frame(distribution_type = c(rep("Replication",length(replications)),
                                       rep("Geometric Sample",length(geom_sample))),
                 samples = c(replications, geom_sample))

library(ggplot2)

# Set the bin-width based on Freedman-Diaconis rule
num_bins <- 2 * IQR(geom_sample) / length(geom_sample)^(1/3)

# Plot two histograms for comparing each distribution type
ggplot(data = df, aes(x=samples)) +
     geom_histogram(binwidth = num_bins) +
     facet_wrap(~distribution_type)
```

**Problem**: A new machine arrives in a factory. This type of machine is very unreliable: every day, it has a 10% chance of breaking permanently. How long would you expect it to last?  

```{r Geometric Distribution Probabilities}
# Find the probability the machine breaks on 5th day or earlier
pgeom(4, .1)

# Find the probability the machine is still working on 20th day
1 - pgeom(19, .1)
```

```{r Graphing Geometric Probabilities}
# Calculate the probability of machine working on day 1-30
still_working <- 1 - pgeom(0:29, .1)

# Plot the probability for days 1 to 30
qplot(1:30, still_working)
```